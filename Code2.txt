#Import useful modules
import os
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
%matplotlib inline

#Set directory and import the Letters data
os.chdir('D:\MSc2\Winter\STA4273\Project')
data = pd.read_csv('Mushroom.txt',header=None)

data.head()
data.shape
data.describe()
data.info()
data.columns.values #column names
#################################################################
##Section 1: Data cleaning and Exploratory Data Analysis
#Rename the columns 
data.columns = ["Label"] + ["Feature" + str(num1) for num1 in range(1,23)]

#Remove feature 11 (no missing values, (8124, 22))
data = data.drop(['Feature11'], axis=1)
data.columns = ["Label"] + ["Feature" + str(num1) for num1 in range(1,22)]

#Create features and labels
y = data['Label'].astype("category")
x = pd.get_dummies(data.iloc[:,1:]) #The explanatory variable (8124, 112)

#Plot the Class Label distributions and the Feature distributions
#Class label distribution
label_dist = y.value_counts().sort_index() #create a table of counts
label_dist.plot.bar()

#Plot all 21 Feature distribution
#platte = ["silver","rosybrown","firebrick","sienna","sandybrown","tan","darkcyan","deepskyblue","royalblue","plum","orange",
#	"yellowgreen","dodgerblue","violet","indianred","lightsteelblue","silver","rosybrown","firebrick","sienna","sandybrown"]
#f = plt.figure(figsize=(13,13))
#for i in range(1,22): 
#	x = data.iloc[:,i]
#	plt.subplot(4, 6, i)
#	plt.hist(x,normed=True,bins=15,color=platte[i-1])
#	plt.title('Feature ' + str(i))

#Split the dataset into training (70%) and testing (30%) dataset
n = data.shape[0]
k = 0.70
q = int(round(n*k))
train_x = x.iloc[:q,:] 
train_y = y[:q]
test_x = x.iloc[q:,:] 
test_y = y[q:]
###############################################################
##Section 2: Build a SVM Classifier
from sklearn.svm import SVC

#Tune the hyperparameters using grid search & cross validation (5 folds)
#C = np.arange(0.01,20,5) #Penalty
Gamma = np.arange(0.03,0.05,5) #Spread of Kernel
K = 5 #5 folds
error = []

for j in range(1,len(Gamma)+1): 
    error2 = []
    for i in range(1,K+1): 
        N = int(train_x.shape[0]/K)
        validateX = train_x.iloc[N*(i-1):N*i,:]
        validateY = train_y.iloc[N*(i-1):N*i]
        trainX = train_x.iloc[0:N*(i-1),:].append(train_x.iloc[N*i:,:])
        trainY = train_y.iloc[0:N*(i-1)].append(train_y.iloc[N*i:])
        SVM = SVC(C=C[k-1],kernel='sigmoid',gamma=Gamma[j-1])
        SVM.fit(trainX, trainY) 
        y_pred = SVM.predict(validateX)
        error2.append(((validateY != y_pred).sum())/validateY.shape[0])
    error.append(sum(error2)/len(error2))

a = np.argmin(error)+1 #"Best" hyperparameter index
b = a % len(Gamma)
param2 = Gamma[b-1]
#c = (a // len(C))+1
#param1 = C[c-1]
#####################################
#Fit model using the tuned parameter using CV validation
SVM = SVC(kernel='sigmoid',gamma=param2)
SVM.fit(train_x,train_y)

#Prediction and Error rate
y_pred1 = SVM.predict(test_x)
a = (test_y != y_pred1).sum()/len(test_y)
print("The testing error of the SVM classifier is: ", a*100) 


